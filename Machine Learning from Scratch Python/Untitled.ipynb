{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "**Machine Learning From Scratch | https://www.youtube.com/watch?v=4swNt7PiamQ&list=PL1jB36QHTjD2FjMC-ReFgwVyHMTDlMCU7&index=1**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y} = wx+b$     \n",
    "\n",
    "This is a line equation, where:    \n",
    "                                            w : Weights (Slope)    \n",
    "                                            b : Bias or Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function - Mean Squared Error (MSE)\n",
    "Difference between the actual value and the approximated values.\n",
    "\n",
    "$ \\mbox{MSE} = J(w,b) = \\frac{1}{N} \\sum_{i=1}^{n} \\big(y_i - (wx_i + b)\\big)^2$\n",
    "\n",
    "We need to keep the Error as small as possible. We need to calculate the minimum of this function, and we find that by calculating the derivitive/gradient of cost function w.r.t. $w$ and $b$:\n",
    "\n",
    "$ J^{'}(m,b) = \\begin{bmatrix} \\frac{df}{dw} \\\\ \\frac{df}{db} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{N} \\sum -2x_i (y_i - (wx_i + b ))\\\\ \\frac{1}{N} \\sum -2 (y_i - (wx_i + b )) \\end{bmatrix} $\n",
    "\n",
    "We use a technique called gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Gradient Descent is an iterative method to get to a minimum. On the cost function, we start on an initial weight or bias, and we go into the direction of steepest bias (the negative direction of gradient), until we reach minimum. With each iteration, we update the weights and biases to new values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rules\n",
    "$w_\\mbox{new} = w_\\mbox{old} - \\alpha \\cdot dw$    \n",
    "$b_\\mbox{new} = b_\\mbox{old} - \\alpha \\cdot db$\n",
    "\n",
    "Where:    \n",
    "          $\\alpha = $ Learning Rate\n",
    "          \n",
    "Small alpha rates might result in slow performance. \n",
    "Big alpha rates might lead to overshooting the minimum continuously, and might never reach to minimum.\n",
    "\n",
    "$\\frac{dJ}{dw} = dw = \\frac{1}{N} \\sum_{i=1}^{n} -2x_i \\big( y_i - (wx_i + b) \\big) = \\frac{1}{N} \\sum_{i=1}^{n} -2x_i(y_i - \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{n} 2x_i(\\hat{y} - y_i)$\n",
    "\n",
    "$\\frac{dJ}{db} = db = \\frac{1}{N} \\sum_{i=1}^{n} -2 \\big( y_i - (wx_i + b) \\big) = \\frac{1}{N} \\sum_{i=1}^{n} -2(y_i - \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{n} 2(\\hat{y} - y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression :\n",
    "    def __init__(self, learningRate = 0.001, numberOfIterations = 1000):\n",
    "        self.learningRate = learningRate\n",
    "        self.numberOfIterations = numberOfIterations\n",
    "        self.weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
